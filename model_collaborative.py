from __future__ import annotations

import pickle
from collections import defaultdict
from pathlib import Path

import pandas as pd
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split


# Minimal path helper (no external package)
ROOT = Path(__file__).resolve().parent
DATA_PROCESSED = ROOT / "data" / "processed"
ARTIFACTS = ROOT / "artifacts"


def precision_recall_at_k(predictions, k=10, threshold=3.5):
    """Return precision and recall at k metrics for each user."""

    user_est_true = defaultdict(list)
    for uid, _, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))

    precisions = {}
    recalls = {}

    for uid, user_ratings in user_est_true.items():
        user_ratings.sort(key=lambda x: x[0], reverse=True)

        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)
        n_rel_and_rec_k = sum(
            ((true_r >= threshold) and (est >= threshold))
            for (est, true_r) in user_ratings[:k]
        )

        precisions[uid] = n_rel_and_rec_k / k if k else 0
        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel else 0

    return precisions, recalls


def main():
    processed_dir = DATA_PROCESSED
    artifacts_dir = ARTIFACTS

    processed_dir.mkdir(parents=True, exist_ok=True)
    artifacts_dir.mkdir(parents=True, exist_ok=True)

    print("üöÄ ƒêANG KH·ªûI T·∫†O MODULE COLLABORATIVE FILTERING (SVD)...")

    # 1. LOAD D·ªÆ LI·ªÜU
    try:
        ratings = pd.read_csv(processed_dir / "ratings_final.csv")
        print(f"‚úÖ ƒê√£ t·∫£i: {ratings.shape[0]} d√≤ng ratings.")
    except FileNotFoundError:
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file 'ratings_final.csv'.")
        raise

    # 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU CHO SURPRISE
    print("‚è≥ ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu...")
    reader = Reader(rating_scale=(0.5, 5))
    data = Dataset.load_from_df(ratings[["userId", "id", "rating"]], reader)

    # 3. CHIA T·∫¨P TRAIN/TEST
    trainset, testset = train_test_split(data, test_size=0.2)

    # 4. HU·∫§N LUY·ªÜN MODEL SVD
    print("‚è≥ ƒêang hu·∫•n luy·ªán thu·∫≠t to√°n SVD (Matrix Factorization)...")
    svd = SVD()
    svd.fit(trainset)

    # 5. ƒê√ÅNH GI√Å
    print("‚è≥ ƒêang t√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√° (RMSE, MAE, Precision@K, Recall@K)...")
    predictions = svd.test(testset)
    rmse = accuracy.rmse(predictions, verbose=False)
    mae = accuracy.mae(predictions, verbose=False)

    precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=3.5)
    avg_precision = sum(precisions.values()) / len(precisions) if precisions else 0.0
    avg_recall = sum(recalls.values()) / len(recalls) if recalls else 0.0

    print("\nüìä B·∫¢NG ƒê√ÅNH GI√Å HI·ªÜU NƒÇNG MODEL (Ghi v√†o b√°o c√°o):")
    print(f"   - RMSE (Sai s·ªë b√¨nh ph∆∞∆°ng): {rmse:.4f}")
    print(f"   - MAE (Sai s·ªë tuy·ªát ƒë·ªëi):    {mae:.4f}")
    print(f"   - Precision@10:              {avg_precision:.4f}")
    print(f"   - Recall@10:                 {avg_recall:.4f}")
    print("   *(Precision th·∫•p l√† b√¨nh th∆∞·ªùng v·ªõi dataset th∆∞a)*")

    # 6. RETRAIN FULL + SAVE
    print("\n‚è≥ ƒêang Retrain tr√™n to√†n b·ªô 100% d·ªØ li·ªáu ƒë·ªÉ l∆∞u Model...")
    full_trainset = data.build_full_trainset()
    svd.fit(full_trainset)

    artifacts_dir.mkdir(parents=True, exist_ok=True)
    model_file = artifacts_dir / "svd_model.pkl"
    pickle.dump(svd, open(model_file, "wb"))
    print(f"‚úÖ HO√ÄN T·∫§T! ƒê√£ l∆∞u model SVD v√†o: '{model_file}'")


if __name__ == "__main__":
    main()
